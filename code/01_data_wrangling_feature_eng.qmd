---
title: "Data Science Applied to Ag - Final Project - ML"
format:
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Md Shakir Moazzem, Umar Munir
---

# Introduction  

This script contains the data wrangling and feature engineering steps for the final project.

# Start of data wrangling

## Loading packages  

The following code chunk will load necessary packages.

```{r Setup, message=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("readxl")
#install.packages("janitor")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("readr")
#install.packages("lubridata")
#install.packages("stringr")

# Loading packages 

library(tidyverse)
library(readxl) # to read excel files
library(janitor) # to clean data; helps fix and standardize the column names
library(dplyr) # wrangling
library(tidyr) # wrangling
library(readr) # to export csv
library(lubridate)
library(stringr)

```


## Reading "training" data  

The following code chunk will read the csv files for the 3 training data sets

```{r training data import, message=F, warning=F}

#reading the csv files for the 3 training data sets 

trait_df <- read_csv("../data/training/training_trait.csv") 
meta_df  <- read_csv("../data/training/training_meta.csv")
soil_df  <- read_csv("../data/training/training_soil.csv")

```



## Data wrangling on "training" data

The code below creates a function to adjust the yield_mg_ha for 15.5% grain moisture.

```{r}

# Function to transform yield to 15.5% moisture
adjust_yield <- function(yield_mg_ha, grain_moisture) {
  yield_mg_ha * (100 - grain_moisture) / (100 - 15.5)
}

```


The code below conducts data wrangling on "trait_df"

```{r clean_and_summarize_trait, message=F, warning=F}



trait_clean <- trait_df %>%
  select(-block) %>%
  mutate(
    site = str_remove_all(site, "[a-z]"), # removing all lowercase letters from site
    site = str_replace(site, "-.*$", ""), # keeping only text before any dash
    site = str_replace(site, "_.*$", "") # keeping only text before any underscore
  ) %>%
  group_by(year, site, hybrid) %>%
  summarize(
    yield_mg_ha    = mean(yield_mg_ha, na.rm = TRUE),
    grain_moisture = mean(grain_moisture, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    adjusted_yield = adjust_yield(yield_mg_ha, grain_moisture),
  ) %>%
  select(-yield_mg_ha, -grain_moisture) %>%
  ungroup()


trait_clean

```


The following code chunk will conduct data wrangling on  "training_soil.csv"


```{r data_wrangling_training_soil, message=F, warning=F}

soil_clean <- soil_df %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  ungroup()

soil_clean
```


The following code chunk will conduct data wrangling on  "training_meta.csv".

```{r clean_meta_sites}

meta_clean <- meta_df %>%
  rename(
    lon = longitude,
    lat = latitude
  ) %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  distinct(year, site, .keep_all = TRUE) %>%
  select(-previous_crop) %>%
  clean_names() %>%
  ungroup() 
  

meta_clean

```



## EDA for cleaned data


```{r}
summary(trait_clean)

summary(soil_clean)

summary(meta_clean)
```



## Merging all 3 cleaned data frames for training data


```{r}

merged_clean  <- trait_clean %>%
  left_join(soil_clean, by = c("year", "site")) %>%
  left_join(meta_clean, by = c("year", "site"))

merged_clean
```

## EDA for cleaned merged data

```{r}
summary(merged_clean)
```


# Open Source Daymet weather data download

The following code chunk will load necessary packages.  

```{r Setup, message=F, warning=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("sf") #to manipulate vector geospatial files
#install.packages("daymetr") #to retrieve data from daymet database website through R
#install.packages("remotes") #to install R packages that are not available on CRAN, and available on GitHub
#remotes::install_github("ropensci/USAboundaries") 
#remotes::install_github("ropensci/USAboundariesData")

# Loading packages

library(tidyverse) #need to load "tidyverse" package at first
library(sf) # for US map #to manipulate vector geo-spatial points
library(daymetr) #to retrieve data from daymet database website through R
library(remotes)
library(USAboundaries) # for US state boundaries
library(USAboundariesData)

```


The following code chunk will create a map of the USA and plot the sites in the map based on their latitude and longitude.


```{r create map of USA and add points, message=F, warning=F}

states <- us_states() %>% 
  filter( !(state_abbr %in% c("PR", "AK", "HI")) ) #to remove "PR" (Puerto Rico), "AK" (Alaska), and "HI" (Hawaii)
  
ggplot() +
  geom_sf(data = states) + #"geom_sf()" is used to plot "sf" object, which we just created above as "states" object; plots all states and territories of USA
  geom_point(data = merged_clean,
             aes(x = lon, #"Longitude" goes on longitude
                 y = lat) #"Latitude" goes on latitude
             ) +
  labs(
    title = "Corn Trial Site Locations (2014–2023)",
    x     = "Longitude",
    y     = "Latitude"
  )

```

The following code chunk will keep the observations for site-years having latitude and longitude withing the Daymet range of co-ordinates.

Declaration of AI use: the following code chunk was inspired and subsequently modified on the basis of code initially generated by ChatGPT


```{r}

# Define Daymet bounding box (WGS-84)
min_lat <- 14.53
max_lat <- 52.00
min_lon <- -131.104
max_lon <- -52.95

# Filter merged to Daymet’s valid range, dropping any NA coords
merged_daymet <- merged_clean %>%
  filter(
    !is.na(lat),
    !is.na(lon),
    lat  >= min_lat,
    lat  <= max_lat,
    lon  >= min_lon,
    lon  <= max_lon
  )

# Report how many rows remain (and were dropped)
message("Rows kept: ", nrow(merged_daymet), 
        " (dropped: ", nrow(merged_clean) - nrow(merged_daymet), ")")


```
The following code chunk will extract unique combinations of year, site, and their coordinates.

```{r unique_site_years_with_coords}

site_year_df <- merged_daymet %>%
  select(year, site, lon, lat) %>%  # need to include longitude and latitude along with site-years
  distinct() %>%                    
  arrange(year, site)

site_year_df

```

The following code chunk will download the weather data for all unique combinations of year, site, and their coordinates in the "site_year_df" object.

```{r}

weather_daymet_all <- site_year_df %>% 
  mutate(weather = pmap(list(.y = year, 
                             .site = site, 
                             .lat = lat, 
                             .lon = lon), 
                        function(.y, .site, .lat, .lon) 
                          download_daymet( 
                            site = .site, 
                            lat = .lat, #specifying ".lat" placeholder for "lat = " argument
                            lon = .lon, #specifying ".lon" placeholder for "lon = " argument
                            start = .y, 
                            end = .y, 
                            simplify = T,
                            silent = T) %>% #end of " download_daymet()" function
                          rename(.year = year,
                                 .site = site) 
                        )) 


weather_daymet_all

```


The following code chunk will unnest the weather column. 

```{r}

weather_daymet_unnest <- weather_daymet_all %>%
  select(year, site, weather) %>% 
  unnest(weather) %>% 
  pivot_wider(names_from = measurement, 
              values_from = value) %>% 
  janitor::clean_names()

weather_daymet_unnest

View(weather_daymet_unnest)

```


# Merging the weather data retrieved from Daymet to the "merged_clean" data

The following code chunk will merge the weather data retrieved from Daymet to the "merged_clean" data

```{r merge_weather_with_data}

# Join daily weather onto your cleaned trial data by year & site

daymet_all_unnest <- merged_clean %>%
  left_join(
    weather_daymet_unnest,
    by = c("year", "site")
  )

fieldweather <- daymet_all_unnest

fieldweather

View(fieldweather)

```


# Feature engineering

## Loading packages

```{r}

#install.packages("ggridges")

library(ggridges)
library(tidyverse)

```


The following code chunk will keep desired variables that we will use further and get abbreviated month name based on the date.

```{r}

fe_month <- fieldweather %>%
  # Selecting needed variables
  dplyr::select(year, 
                site, 
                hybrid, 
                lon, 
                lat, 
                yday, 
                yield = adjusted_yield, 
                soil.ph = soilpH, 
                soil.om.pct = om_pct, 
                soil.k.ppm = soilk_ppm, 
                soil.p.ppm = soilp_ppm, 
                dayl.s = dayl_s, #to rename variable name from "dayl_s" to dayl.s
                prcp.mm = prcp_mm_day, #to rename variable name to "prcp.mm"
                srad.wm2 = srad_w_m_2,#to rename variable name to "srad.wm2"
                tmax.c = tmax_deg_c, #to rename variable name to "tmax.c"
                tmin.c = tmin_deg_c,#to rename variable name to "tmin.c"
                vp.pa = vp_pa #to rename variable name to "vp.pa"
                ) %>%
  # Creating a date class variable  
  mutate(date_chr = paste0(year, "/", yday)) %>% 
  mutate(date = as.Date(date_chr, "%Y/%j")) %>% 
  # Extracting month from date  
  mutate(month = month(date)) %>% 
  mutate(month_abb = month(date, label = T)) #To get abbreviated month name e.g., Jan, Feb, Mar,...,Dec #month_abb is in "ord" (ordinal) format

fe_month
```


The following code chunk will summarize daily weather variables based on month.  

```{r fe_month_sum}

fe_month_sum <- fe_month %>%
  group_by(year, site, hybrid, month_abb, yield) %>% #If we do a summarise() after group_by(), any column that's not in the group_by() is gone, so we need to include "yield" in the group_by() to include it in the data frame because "yield" is our response variable so we must keep it in the data frame 
  #Because we are gonna be applying a "summarize()" function to different columns, we are gonna use a function called "across()" 
  summarise(across(.cols = c(soil.ph, 
                             soil.om.pct, 
                             soil.k.ppm, 
                             soil.p.ppm,
                             dayl.s,
                             srad.wm2,
                             tmax.c,
                             tmin.c,
                             vp.pa),
                   .fns = mean, #do not indicate the actual function "mean()", just use the word "mean"
                   .names = "mean_{.col}"), #specifying the weather variables that we want their mean as new column variables #1st across() is applying the "mean" function (to summarize "mean")
            across(.cols = prcp.mm,
                   .fns = sum,
                   .names = "sum_{.col}"
                   ) #specifying the weather variable (prcp.mm) that we want its sum as new column variable #2nd across() is summarizing sum #2nd across() is applying the "sum" function (to summarize "sum")
            ) %>%
  ungroup() #To convert from "group" to "tibble"


fe_month_sum

```


The following code chunk will check "tmax.c" and "prcp.mm" for the first site-year and month for the purpose of double checking to make sure that we did everything okay the way we intended. 

```{r}

fe_month %>%
  filter(year == 2014 & 
           site == "DEH1" &
           hybrid == "B37/MO17" &
           month_abb == "Jan") %>%
  summarise(tmax.c = mean(tmax.c),
            prcp.mm = sum(prcp.mm))


```

The code below will put the month as part of the column name. 


```{r fe_month_sum_wide}

fe_month_sum_wide <- fe_month_sum %>%
  pivot_longer(cols = mean_soil.ph:sum_prcp.mm) %>% 
  mutate(varname = paste0(name, "_", month_abb)) %>% 
  dplyr::select(-name, -month_abb) %>% 
  pivot_wider(names_from = varname,
              values_from = value) %>%
  # Rounding to one decimal point
  mutate(across(c(4:ncol(.)), ~round(., 1) )) %>%
  select( -mean_soil.ph_NA, -mean_soil.om.pct_NA, -mean_soil.k.ppm_NA, -mean_soil.p.ppm_NA, -mean_dayl.s_NA, -mean_srad.wm2_NA, -mean_tmax.c_NA, -mean_tmin.c_NA, -mean_vp.pa_NA, -sum_prcp.mm_NA)

fe_month_sum_wide  

```


The following code chunk will make a ridge plot to visualize the distribution of one variable over months.  

```{r, message=FALSE, warning=FALSE}

#install.packages("ggridges")
library(ggridges) #really powerful package to plot distributions e.g., density plot

ggplot(data = fe_month_sum,
       aes(x = mean_tmax.c,
           y = month_abb,
           fill = stat(x) #fill = stat(x): specific to "ggridges"
           )
       ) +
  geom_density_ridges_gradient(scale = 3,
                               rel_min_height = 0.01) + #"the 2nd argument is"rel_min_height = 0.01": to cut down the tails of the distribution to look nice
  scale_fill_viridis_c(option = "C") + #"viridis"scale_fill_viridis_c()": color-blind-friendly color scale #argument (option = "C") to change the color to "magma" because magma is weather related; "option = " varies from A to F
  theme(legend.position = "none") #"theme(legend.position = "none")": to remove the legend from the graph

```


The following code chunk will automate the plotting of ridge plots for all weather variables.

```{r}

finalplots <- fe_month_sum %>%
  pivot_longer(cols = mean_soil.ph:sum_prcp.mm) %>% #we need to do pivot_longer() to iterate over the weather variables
  group_by(name) %>%
  nest() %>% #for iteration, we use "group_by()" followed by "nest()" in combo
  #we will use map2() since we need to iterate over 2 columns
  mutate(plot = map2(data, name, #map2() takes 2 arguments: the 1st argument becomes .x, the 2nd argument becomes .y #we must use map2() with a mutate() at first for iteration #"map2(data, name" : we want to iterate over "name" for "data"
                     ~ ggplot( data = .x, # .x represent "data" in the map2() function, so we need to use data = .x [= "data" from map2() ] to feed the "data" from map2() function as the data of ggplot() #.x is the iterating column of map2() that is a place holder for the 1st argument of map2()
       aes(x = value,
           y = month_abb,
           fill = stat(x)
           )
       ) +
  geom_density_ridges_gradient(scale = 3,
                               rel_min_height = 0.01) + 
  scale_fill_viridis_c(option = "C") + 
  theme(legend.position = "none") +
  labs(x = .y) #to rename the x-axis as the variable name # .y is the placeholder for "name" (i.e., variable name) in map2() function
                     )) 
  
finalplots

finalplots$plot

```

The following code chunk will export the feature engineered data file as .csv in the local disk, which we will use in our further data analysis.

```{r}

write_csv(fe_month_sum_wide,
           "../data/weather_monthsum.csv")

```

